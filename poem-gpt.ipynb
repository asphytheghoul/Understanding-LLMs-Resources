{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-03-15T13:14:50.975469Z","iopub.status.busy":"2023-03-15T13:14:50.975013Z","iopub.status.idle":"2023-03-15T13:15:02.522522Z","shell.execute_reply":"2023-03-15T13:15:02.521446Z","shell.execute_reply.started":"2023-03-15T13:14:50.975429Z"},"trusted":true},"outputs":[],"source":["import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\""]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-03-15T13:15:02.525541Z","iopub.status.busy":"2023-03-15T13:15:02.524519Z","iopub.status.idle":"2023-03-15T13:15:12.549798Z","shell.execute_reply":"2023-03-15T13:15:12.548732Z","shell.execute_reply.started":"2023-03-15T13:15:02.525498Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ed87f88bee79411eaf52fcd32e2347fd","version_major":2,"version_minor":0},"text/plain":["Downloading (â€¦)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5f89ad0e6f204a629f915445dc8777af","version_major":2,"version_minor":0},"text/plain":["Downloading (â€¦)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3ab716d6cb0447cca3aa27cc3808d9de","version_major":2,"version_minor":0},"text/plain":["Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3f76505dacba4d7684f1cce5783d1758","version_major":2,"version_minor":0},"text/plain":["Downloading (â€¦)\"pytorch_model.bin\";:   0%|          | 0.00/548M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6b9d36a4e320400da4e79ea9e01a7da6","version_major":2,"version_minor":0},"text/plain":["Downloading (â€¦)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","model = GPT2LMHeadModel.from_pretrained('gpt2')\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-03-15T13:15:12.552461Z","iopub.status.busy":"2023-03-15T13:15:12.551414Z","iopub.status.idle":"2023-03-15T13:16:05.990641Z","shell.execute_reply":"2023-03-15T13:16:05.989599Z","shell.execute_reply.started":"2023-03-15T13:15:12.552418Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n","  FutureWarning,\n"]}],"source":["import os\n","\n","# Define the path to the root directory containing all the TXT files\n","root_path = '/kaggle/input/poemsdataset/forms'\n","\n","# Get a list of all the TXT files in the root directory and its subdirectories\n","file_list = []\n","for dirpath, dirnames, filenames in os.walk(root_path):\n","    for filename in filenames:\n","        if filename.endswith('.txt'):\n","            file_list.append(os.path.join(dirpath, filename))\n","\n","# Load the tokenizer\n","tokenizer = tokenizer\n","\n","# Concatenate all the text data into a single file\n","output_file = '/kaggle/working/poemd.txt'\n","with open(output_file, 'w') as f:\n","    for file_path in file_list:\n","        with open(file_path, 'r') as f_in:\n","            f.write(f_in.read())\n","\n","# Create a TextDataset object from the concatenated text data file\n","dataset = TextDataset(\n","    tokenizer=tokenizer,\n","    file_path=output_file,\n","    block_size=356\n",")\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-03-15T13:16:05.993785Z","iopub.status.busy":"2023-03-15T13:16:05.993137Z","iopub.status.idle":"2023-03-15T13:16:05.999368Z","shell.execute_reply":"2023-03-15T13:16:05.998043Z","shell.execute_reply.started":"2023-03-15T13:16:05.993744Z"},"trusted":true},"outputs":[],"source":["import os\n","\n","# Define the path to the directory containing all the TXT files\n","dir_path = '/kaggle/input/poemsdataset/forms/carol'\n","\n","# Get a list of all the TXT files in the directory\n","file_list = [os.path.join(dir_path, f) for f in os.listdir(dir_path) if f.endswith('.txt')]\n","\n","# Load the tokenizer\n","tokenizer = tokenizer\n","\n","# Concatenate all the text data into a single file\n","output_file = '/kaggle/working/poemd.txt'\n","with open(output_file, 'w') as f:\n","    for file_path in file_list:\n","        with open(file_path, 'r') as f_in:\n","            f.write(f_in.read())\n","\n","# Create a TextDataset object from the concatenated text data file\n","dataset = TextDataset(\n","    tokenizer=tokenizer,\n","    file_path=output_file,\n","    block_size=512\n",")\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-03-15T13:16:06.001240Z","iopub.status.busy":"2023-03-15T13:16:06.000775Z","iopub.status.idle":"2023-03-15T13:16:06.014306Z","shell.execute_reply":"2023-03-15T13:16:06.013332Z","shell.execute_reply.started":"2023-03-15T13:16:06.001203Z"},"trusted":true},"outputs":[],"source":["data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer, \n","    mlm=False\n",")\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-03-15T13:16:06.016450Z","iopub.status.busy":"2023-03-15T13:16:06.015525Z","iopub.status.idle":"2023-03-15T13:16:06.095049Z","shell.execute_reply":"2023-03-15T13:16:06.093826Z","shell.execute_reply.started":"2023-03-15T13:16:06.016412Z"},"trusted":true},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir='./results',           # output directory\n","    num_train_epochs=10,               # total number of training epochs\n","    per_device_train_batch_size=16,   # batch size per device during training\n","    per_device_eval_batch_size=32,    # batch size for evaluation\n","    warmup_steps=500,                 # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,                # strength of weight decay\n","    logging_dir='./logs',             # directory for storing logs\n","    logging_steps=100,    # number of steps between logging updates\n","    learning_rate=5e-5\n",")\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-03-15T13:16:06.097430Z","iopub.status.busy":"2023-03-15T13:16:06.096749Z","iopub.status.idle":"2023-03-15T13:16:10.201109Z","shell.execute_reply":"2023-03-15T13:16:10.200051Z","shell.execute_reply.started":"2023-03-15T13:16:06.097391Z"},"trusted":true},"outputs":[],"source":["trainer = Trainer(\n","    model=model,                      \n","    args=training_args,                \n","    data_collator=data_collator,        \n","    train_dataset=dataset\n",")\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-03-15T13:16:10.202706Z","iopub.status.busy":"2023-03-15T13:16:10.202346Z","iopub.status.idle":"2023-03-15T14:11:38.097860Z","shell.execute_reply":"2023-03-15T14:11:38.096920Z","shell.execute_reply.started":"2023-03-15T13:16:10.202662Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 6190\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3870\n","  Number of trainable parameters = 124439808\n","Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["wandb version 0.14.0 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.13.10"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20230315_131646-17cwuwfe</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/p1234ppp/huggingface/runs/17cwuwfe' target=\"_blank\">splendid-vortex-9</a></strong> to <a href='https://wandb.ai/p1234ppp/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/p1234ppp/huggingface' target=\"_blank\">https://wandb.ai/p1234ppp/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/p1234ppp/huggingface/runs/17cwuwfe' target=\"_blank\">https://wandb.ai/p1234ppp/huggingface/runs/17cwuwfe</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3870' max='3870' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3870/3870 54:17, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>3.963400</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>3.835600</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>3.698400</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>3.641300</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>3.593300</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>3.605700</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>3.542900</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>3.516800</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>3.464700</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>3.471600</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>3.440900</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>3.432300</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>3.363000</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>3.343200</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>3.399600</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>3.320700</td>\n","    </tr>\n","    <tr>\n","      <td>1700</td>\n","      <td>3.325500</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>3.307900</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>3.267600</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>3.297800</td>\n","    </tr>\n","    <tr>\n","      <td>2100</td>\n","      <td>3.248000</td>\n","    </tr>\n","    <tr>\n","      <td>2200</td>\n","      <td>3.223200</td>\n","    </tr>\n","    <tr>\n","      <td>2300</td>\n","      <td>3.253600</td>\n","    </tr>\n","    <tr>\n","      <td>2400</td>\n","      <td>3.180700</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>3.248400</td>\n","    </tr>\n","    <tr>\n","      <td>2600</td>\n","      <td>3.173800</td>\n","    </tr>\n","    <tr>\n","      <td>2700</td>\n","      <td>3.259200</td>\n","    </tr>\n","    <tr>\n","      <td>2800</td>\n","      <td>3.151900</td>\n","    </tr>\n","    <tr>\n","      <td>2900</td>\n","      <td>3.160700</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>3.183500</td>\n","    </tr>\n","    <tr>\n","      <td>3100</td>\n","      <td>3.210600</td>\n","    </tr>\n","    <tr>\n","      <td>3200</td>\n","      <td>3.115900</td>\n","    </tr>\n","    <tr>\n","      <td>3300</td>\n","      <td>3.214200</td>\n","    </tr>\n","    <tr>\n","      <td>3400</td>\n","      <td>3.162600</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>3.116200</td>\n","    </tr>\n","    <tr>\n","      <td>3600</td>\n","      <td>3.142200</td>\n","    </tr>\n","    <tr>\n","      <td>3700</td>\n","      <td>3.112700</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>3.137300</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Configuration saved in ./results/checkpoint-500/generation_config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n","Saving model checkpoint to ./results/checkpoint-1000\n","Configuration saved in ./results/checkpoint-1000/config.json\n","Configuration saved in ./results/checkpoint-1000/generation_config.json\n","Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n","Saving model checkpoint to ./results/checkpoint-1500\n","Configuration saved in ./results/checkpoint-1500/config.json\n","Configuration saved in ./results/checkpoint-1500/generation_config.json\n","Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n","Saving model checkpoint to ./results/checkpoint-2000\n","Configuration saved in ./results/checkpoint-2000/config.json\n","Configuration saved in ./results/checkpoint-2000/generation_config.json\n","Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n","Saving model checkpoint to ./results/checkpoint-2500\n","Configuration saved in ./results/checkpoint-2500/config.json\n","Configuration saved in ./results/checkpoint-2500/generation_config.json\n","Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n","Saving model checkpoint to ./results/checkpoint-3000\n","Configuration saved in ./results/checkpoint-3000/config.json\n","Configuration saved in ./results/checkpoint-3000/generation_config.json\n","Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n","Saving model checkpoint to ./results/checkpoint-3500\n","Configuration saved in ./results/checkpoint-3500/config.json\n","Configuration saved in ./results/checkpoint-3500/generation_config.json\n","Model weights saved in ./results/checkpoint-3500/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"data":{"text/plain":["TrainOutput(global_step=3870, training_loss=3.3423165925097402, metrics={'train_runtime': 3327.8567, 'train_samples_per_second': 18.601, 'train_steps_per_second': 1.163, 'total_flos': 1.12459682304e+16, 'train_loss': 3.3423165925097402, 'epoch': 10.0})"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-03-15T14:16:12.283489Z","iopub.status.busy":"2023-03-15T14:16:12.282752Z","iopub.status.idle":"2023-03-15T14:16:13.240160Z","shell.execute_reply":"2023-03-15T14:16:13.238796Z","shell.execute_reply.started":"2023-03-15T14:16:12.283451Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Configuration saved in ./gpt2_256_full/config.json\n","Configuration saved in ./gpt2_256_full/generation_config.json\n","Model weights saved in ./gpt2_256_full/pytorch_model.bin\n"]}],"source":["model.save_pretrained(\"./gpt2_256_full/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-03-13T18:28:13.423971Z","iopub.status.idle":"2023-03-13T18:28:13.427109Z","shell.execute_reply":"2023-03-13T18:28:13.426813Z","shell.execute_reply.started":"2023-03-13T18:28:13.426782Z"},"trusted":true},"outputs":[],"source":["# trainer.evaluate()"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-03-15T14:24:03.892517Z","iopub.status.busy":"2023-03-15T14:24:03.891764Z","iopub.status.idle":"2023-03-15T14:24:33.899206Z","shell.execute_reply":"2023-03-15T14:24:33.897972Z","shell.execute_reply.started":"2023-03-15T14:24:03.892478Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["loading file vocab.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/vocab.json\n","loading file merges.txt from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/merges.txt\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at None\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"gpt2\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 50256,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"transformers_version\": \"4.26.1\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257\n","}\n","\n","loading configuration file /kaggle/working/gpt2_256_full/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"/kaggle/working/gpt2_256_full\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 50256,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257\n","}\n","\n","loading weights file /kaggle/working/gpt2_256_full/pytorch_model.bin\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at /kaggle/working/gpt2_256_full.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","loading configuration file /kaggle/working/gpt2_256_full/generation_config.json\n","Generate config GenerationConfig {\n","  \"_from_model_config\": true,\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"do_sample\": true,\n","  \"eos_token_id\": 50256,\n","  \"max_length\": 50,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]}],"source":["from transformers import pipeline, GPT2Tokenizer, AutoModelWithLMHead\n","\n","# Load the tokenizer and fine-tuned model\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","model = AutoModelWithLMHead.from_pretrained('/kaggle/working/gpt2_256_full')\n","\n","# Set up the pipeline for generating text\n","poetry_generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n","\n","# Generate a poem based on a prompt\n","prompt = \"Criminal, you took a great piece of my life\"\n","generated_poem = poetry_generator(prompt, max_length=512)\n","\n","# Print the generated poem\n"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-03-15T14:24:33.902398Z","iopub.status.busy":"2023-03-15T14:24:33.901658Z","iopub.status.idle":"2023-03-15T14:24:33.911196Z","shell.execute_reply":"2023-03-15T14:24:33.909941Z","shell.execute_reply.started":"2023-03-15T14:24:33.902358Z"},"trusted":true},"outputs":[],"source":["# generated_poem"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-03-15T14:24:33.913555Z","iopub.status.busy":"2023-03-15T14:24:33.913144Z","iopub.status.idle":"2023-03-15T14:24:33.923845Z","shell.execute_reply":"2023-03-15T14:24:33.922740Z","shell.execute_reply.started":"2023-03-15T14:24:33.913512Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Criminal, you took a great piece of my life -\n","And in it you must repay me.\n","You came in a robe of scarlet black,\n","And at a time of great concern\n","You had my advice, and was ready to answer,\n","For I said no to it;\n","If it be so, then no man will give you any rest,\n","No one will answer me again, if it be so.\n","For you, a fool, who had no sense -\n","I cannot tell - but you left behind me a\n","Sacrament of pain, and a debt unpaid.\n","My Lord, I shall not be sold again -\n","Nor may my dear Lord receive it from me,\n","For I am the son of a prostitute,\n","And that he was no longer my Lord was written\n","in the book of repentance.\n","Your pardon for me must not only be given,\n","Your life is mine, as if you loved me -\n","You gave a sacrifice for my sake;\n","My sins were your own.\n","Oh, it is not so, for he loved and was faithful -\n","I, an innocent virgin,\n","Had been his bride till you began.\n","He loved me, I was his only love\n","For I love you more than his mother did\n","For he was a faithful wife, and that he loved me\n","And that he was not deceived by the wine of my wine.\n","I did not love, nor did you know that he loved more -\n","You said, \"My poor virgin\" - and I answered:\n","And this is when I say: - \"I came from the Lord, and that I loved you more\n","Because you did not change me in my sins;\n","Because you are God's son and that you are my heart-mate.\n","If my sins are forgiven, then I will have given you peace,\"\n","I gave it like a curse or death.\n","Oh, when you do my Lord's work in his sight,\n","He will see, and His eyes will see!And when thou in the city of heaven dost live\n","Thou shalt find a priest and a priestess\n","And her children in the fields\n","And she shall bear unto him her mother's child\n","The little one, even the smallest\n","And she with them the little one\n","But with this shepherdess\n","And their home shall the world adorn\n","And all their food, and all their drink.\n","And that land may well be called the land of gold\n","\n"]}],"source":["print(generated_poem[0]['generated_text'])\n"]},{"cell_type":"markdown","metadata":{},"source":["# INFERENCE V2"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-03-15T14:17:13.865202Z","iopub.status.busy":"2023-03-15T14:17:13.864703Z","iopub.status.idle":"2023-03-15T14:17:16.384721Z","shell.execute_reply":"2023-03-15T14:17:16.379732Z","shell.execute_reply.started":"2023-03-15T14:17:13.865153Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["loading file vocab.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/vocab.json\n","loading file merges.txt from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/merges.txt\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at None\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"gpt2\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 50256,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"transformers_version\": \"4.26.1\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257\n","}\n","\n","loading configuration file /kaggle/working/gpt2_256_full/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"/kaggle/working/gpt2_256_full\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 50256,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257\n","}\n","\n","loading weights file /kaggle/working/gpt2_256_full/pytorch_model.bin\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at /kaggle/working/gpt2_256_full.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","loading configuration file /kaggle/working/gpt2_256_full/generation_config.json\n","Generate config GenerationConfig {\n","  \"_from_model_config\": true,\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n"]}],"source":["import torch\n","from transformers import GPT2Tokenizer, AutoModelWithLMHead\n","\n","# Load the tokenizer and fine-tuned model\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","model = AutoModelWithLMHead.from_pretrained('/kaggle/working/gpt2_256_full')\n","\n","# Set the device for inference\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","# Define a function for generating poems\n","def generate_poem(prompt, max_length=256, temperature=1.0):\n","    # Tokenize the prompt and convert to tensor\n","    prompt_tokens = tokenizer.encode(prompt, add_special_tokens=False)\n","    prompt_tensor = torch.tensor([prompt_tokens]).to(device)\n","\n","    # Generate text using the fine-tuned model\n","    generated_tokens = model.generate(\n","        input_ids=prompt_tensor,\n","        max_length=max_length+len(prompt_tokens),\n","        temperature=temperature,\n","        do_sample=True,\n","        num_beams=5,\n","        top_k=50,\n","        top_p=0.95,\n","        repetition_penalty=1.5,\n","        no_repeat_ngram_size=2,\n","        pad_token_id=tokenizer.eos_token_id\n","    )\n","\n","    # Decode the generated tokens and return the resulting poem\n","    poem = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n","    return poem\n"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-03-15T14:18:29.930805Z","iopub.status.busy":"2023-03-15T14:18:29.930226Z","iopub.status.idle":"2023-03-15T14:18:34.578289Z","shell.execute_reply":"2023-03-15T14:18:34.574940Z","shell.execute_reply.started":"2023-03-15T14:18:29.930763Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n"]},{"ename":"RuntimeError","evalue":"probability tensor contains either `inf`, `nan` or element < 0","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/3295644200.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"There once was a ship that put to sea\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgenerated_poem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_poem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_poem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/3751585683.py\u001b[0m in \u001b[0;36mgenerate_poem\u001b[0;34m(prompt, max_length, temperature)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mno_repeat_ngram_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mpad_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     )\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1520\u001b[0m                 \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m                 \u001b[0msynced_gpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1523\u001b[0m             )\n\u001b[1;32m   1524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mbeam_sample\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3094\u001b[0m             \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3096\u001b[0;31m             \u001b[0mnext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3097\u001b[0m             \u001b[0mnext_token_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"]}],"source":["prompt = \"There once was a ship that put to sea\"\n","generated_poem = generate_poem(prompt, max_length=256, temperature=0.7)\n","print(generated_poem)\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-03-15T14:17:28.606837Z","iopub.status.busy":"2023-03-15T14:17:28.605790Z","iopub.status.idle":"2023-03-15T14:17:56.281680Z","shell.execute_reply":"2023-03-15T14:17:56.280390Z","shell.execute_reply.started":"2023-03-15T14:17:28.606798Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["  adding: kaggle/working/gpt2_256_full/ (stored 0%)\n","  adding: kaggle/working/gpt2_256_full/generation_config.json (deflated 24%)\n","  adding: kaggle/working/gpt2_256_full/pytorch_model.bin (deflated 9%)\n","  adding: kaggle/working/gpt2_256_full/config.json (deflated 51%)\n"]}],"source":["!zip -r gpt2_256 '/kaggle/working/gpt2_256_full'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
