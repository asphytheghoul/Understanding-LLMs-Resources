{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-15T13:14:50.975013Z","iopub.execute_input":"2023-03-15T13:14:50.975469Z","iopub.status.idle":"2023-03-15T13:15:02.522522Z","shell.execute_reply.started":"2023-03-15T13:14:50.975429Z","shell.execute_reply":"2023-03-15T13:15:02.521446Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\n","metadata":{"execution":{"iopub.status.busy":"2023-03-15T13:15:02.524519Z","iopub.execute_input":"2023-03-15T13:15:02.525541Z","iopub.status.idle":"2023-03-15T13:15:12.549798Z","shell.execute_reply.started":"2023-03-15T13:15:02.525498Z","shell.execute_reply":"2023-03-15T13:15:12.548732Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed87f88bee79411eaf52fcd32e2347fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f89ad0e6f204a629f915445dc8777af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ab716d6cb0447cca3aa27cc3808d9de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)\"pytorch_model.bin\";:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f76505dacba4d7684f1cce5783d1758"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b9d36a4e320400da4e79ea9e01a7da6"}},"metadata":{}}]},{"cell_type":"code","source":"import os\n\n# Define the path to the root directory containing all the TXT files\nroot_path = '/kaggle/input/poemsdataset/forms'\n\n# Get a list of all the TXT files in the root directory and its subdirectories\nfile_list = []\nfor dirpath, dirnames, filenames in os.walk(root_path):\n    for filename in filenames:\n        if filename.endswith('.txt'):\n            file_list.append(os.path.join(dirpath, filename))\n\n# Load the tokenizer\ntokenizer = tokenizer\n\n# Concatenate all the text data into a single file\noutput_file = '/kaggle/working/poemd.txt'\nwith open(output_file, 'w') as f:\n    for file_path in file_list:\n        with open(file_path, 'r') as f_in:\n            f.write(f_in.read())\n\n# Create a TextDataset object from the concatenated text data file\ndataset = TextDataset(\n    tokenizer=tokenizer,\n    file_path=output_file,\n    block_size=356\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-15T13:15:12.551414Z","iopub.execute_input":"2023-03-15T13:15:12.552461Z","iopub.status.idle":"2023-03-15T13:16:05.990641Z","shell.execute_reply.started":"2023-03-15T13:15:12.552418Z","shell.execute_reply":"2023-03-15T13:16:05.989599Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n  FutureWarning,\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\n# Define the path to the directory containing all the TXT files\ndir_path = '/kaggle/input/poemsdataset/forms/carol'\n\n# Get a list of all the TXT files in the directory\nfile_list = [os.path.join(dir_path, f) for f in os.listdir(dir_path) if f.endswith('.txt')]\n\n# Load the tokenizer\ntokenizer = tokenizer\n\n# Concatenate all the text data into a single file\noutput_file = '/kaggle/working/poemd.txt'\nwith open(output_file, 'w') as f:\n    for file_path in file_list:\n        with open(file_path, 'r') as f_in:\n            f.write(f_in.read())\n\n# Create a TextDataset object from the concatenated text data file\ndataset = TextDataset(\n    tokenizer=tokenizer,\n    file_path=output_file,\n    block_size=512\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-15T13:16:05.993137Z","iopub.execute_input":"2023-03-15T13:16:05.993785Z","iopub.status.idle":"2023-03-15T13:16:05.999368Z","shell.execute_reply.started":"2023-03-15T13:16:05.993744Z","shell.execute_reply":"2023-03-15T13:16:05.998043Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"data_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, \n    mlm=False\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-15T13:16:06.000775Z","iopub.execute_input":"2023-03-15T13:16:06.001240Z","iopub.status.idle":"2023-03-15T13:16:06.014306Z","shell.execute_reply.started":"2023-03-15T13:16:06.001203Z","shell.execute_reply":"2023-03-15T13:16:06.013332Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='./results',           # output directory\n    num_train_epochs=10,               # total number of training epochs\n    per_device_train_batch_size=16,   # batch size per device during training\n    per_device_eval_batch_size=32,    # batch size for evaluation\n    warmup_steps=500,                 # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,                # strength of weight decay\n    logging_dir='./logs',             # directory for storing logs\n    logging_steps=100,    # number of steps between logging updates\n    learning_rate=5e-5\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-15T13:16:06.015525Z","iopub.execute_input":"2023-03-15T13:16:06.016450Z","iopub.status.idle":"2023-03-15T13:16:06.095049Z","shell.execute_reply.started":"2023-03-15T13:16:06.016412Z","shell.execute_reply":"2023-03-15T13:16:06.093826Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,                      \n    args=training_args,                \n    data_collator=data_collator,        \n    train_dataset=dataset\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-15T13:16:06.096749Z","iopub.execute_input":"2023-03-15T13:16:06.097430Z","iopub.status.idle":"2023-03-15T13:16:10.201109Z","shell.execute_reply.started":"2023-03-15T13:16:06.097391Z","shell.execute_reply":"2023-03-15T13:16:10.200051Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"trainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2023-03-15T13:16:10.202346Z","iopub.execute_input":"2023-03-15T13:16:10.202706Z","iopub.status.idle":"2023-03-15T14:11:38.097860Z","shell.execute_reply.started":"2023-03-15T13:16:10.202662Z","shell.execute_reply":"2023-03-15T14:11:38.096920Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 6190\n  Num Epochs = 10\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 3870\n  Number of trainable parameters = 124439808\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.14.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.13.10"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230315_131646-17cwuwfe</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/p1234ppp/huggingface/runs/17cwuwfe' target=\"_blank\">splendid-vortex-9</a></strong> to <a href='https://wandb.ai/p1234ppp/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/p1234ppp/huggingface' target=\"_blank\">https://wandb.ai/p1234ppp/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/p1234ppp/huggingface/runs/17cwuwfe' target=\"_blank\">https://wandb.ai/p1234ppp/huggingface/runs/17cwuwfe</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3870' max='3870' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3870/3870 54:17, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>3.963400</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>3.835600</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>3.698400</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>3.641300</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>3.593300</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>3.605700</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>3.542900</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>3.516800</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>3.464700</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>3.471600</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>3.440900</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>3.432300</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>3.363000</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>3.343200</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>3.399600</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>3.320700</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>3.325500</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>3.307900</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>3.267600</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>3.297800</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>3.248000</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>3.223200</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>3.253600</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>3.180700</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>3.248400</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>3.173800</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>3.259200</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>3.151900</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>3.160700</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>3.183500</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>3.210600</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>3.115900</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>3.214200</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>3.162600</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>3.116200</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>3.142200</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>3.112700</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>3.137300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./results/checkpoint-500\nConfiguration saved in ./results/checkpoint-500/config.json\nConfiguration saved in ./results/checkpoint-500/generation_config.json\nModel weights saved in ./results/checkpoint-500/pytorch_model.bin\nSaving model checkpoint to ./results/checkpoint-1000\nConfiguration saved in ./results/checkpoint-1000/config.json\nConfiguration saved in ./results/checkpoint-1000/generation_config.json\nModel weights saved in ./results/checkpoint-1000/pytorch_model.bin\nSaving model checkpoint to ./results/checkpoint-1500\nConfiguration saved in ./results/checkpoint-1500/config.json\nConfiguration saved in ./results/checkpoint-1500/generation_config.json\nModel weights saved in ./results/checkpoint-1500/pytorch_model.bin\nSaving model checkpoint to ./results/checkpoint-2000\nConfiguration saved in ./results/checkpoint-2000/config.json\nConfiguration saved in ./results/checkpoint-2000/generation_config.json\nModel weights saved in ./results/checkpoint-2000/pytorch_model.bin\nSaving model checkpoint to ./results/checkpoint-2500\nConfiguration saved in ./results/checkpoint-2500/config.json\nConfiguration saved in ./results/checkpoint-2500/generation_config.json\nModel weights saved in ./results/checkpoint-2500/pytorch_model.bin\nSaving model checkpoint to ./results/checkpoint-3000\nConfiguration saved in ./results/checkpoint-3000/config.json\nConfiguration saved in ./results/checkpoint-3000/generation_config.json\nModel weights saved in ./results/checkpoint-3000/pytorch_model.bin\nSaving model checkpoint to ./results/checkpoint-3500\nConfiguration saved in ./results/checkpoint-3500/config.json\nConfiguration saved in ./results/checkpoint-3500/generation_config.json\nModel weights saved in ./results/checkpoint-3500/pytorch_model.bin\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3870, training_loss=3.3423165925097402, metrics={'train_runtime': 3327.8567, 'train_samples_per_second': 18.601, 'train_steps_per_second': 1.163, 'total_flos': 1.12459682304e+16, 'train_loss': 3.3423165925097402, 'epoch': 10.0})"},"metadata":{}}]},{"cell_type":"code","source":"model.save_pretrained(\"./gpt2_256_full/\")","metadata":{"execution":{"iopub.status.busy":"2023-03-15T14:16:12.282752Z","iopub.execute_input":"2023-03-15T14:16:12.283489Z","iopub.status.idle":"2023-03-15T14:16:13.240160Z","shell.execute_reply.started":"2023-03-15T14:16:12.283451Z","shell.execute_reply":"2023-03-15T14:16:13.238796Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Configuration saved in ./gpt2_256_full/config.json\nConfiguration saved in ./gpt2_256_full/generation_config.json\nModel weights saved in ./gpt2_256_full/pytorch_model.bin\n","output_type":"stream"}]},{"cell_type":"code","source":"# trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2023-03-13T18:28:13.423971Z","iopub.status.idle":"2023-03-13T18:28:13.427109Z","shell.execute_reply.started":"2023-03-13T18:28:13.426782Z","shell.execute_reply":"2023-03-13T18:28:13.426813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline, GPT2Tokenizer, AutoModelWithLMHead\n\n# Load the tokenizer and fine-tuned model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = AutoModelWithLMHead.from_pretrained('/kaggle/working/gpt2_256_full')\n\n# Set up the pipeline for generating text\npoetry_generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n\n# Generate a poem based on a prompt\nprompt = \"Criminal, you took a great piece of my life\"\ngenerated_poem = poetry_generator(prompt, max_length=512)\n\n# Print the generated poem\n","metadata":{"execution":{"iopub.status.busy":"2023-03-15T14:24:03.891764Z","iopub.execute_input":"2023-03-15T14:24:03.892517Z","iopub.status.idle":"2023-03-15T14:24:33.899206Z","shell.execute_reply.started":"2023-03-15T14:24:03.892478Z","shell.execute_reply":"2023-03-15T14:24:33.897972Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"loading file vocab.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/vocab.json\nloading file merges.txt from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/merges.txt\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at None\nloading file tokenizer_config.json from cache at None\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\nModel config GPT2Config {\n  \"_name_or_path\": \"gpt2\",\n  \"activation_function\": \"gelu_new\",\n  \"architectures\": [\n    \"GPT2LMHeadModel\"\n  ],\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"initializer_range\": 0.02,\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 1024,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 12,\n  \"n_positions\": 1024,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"task_specific_params\": {\n    \"text-generation\": {\n      \"do_sample\": true,\n      \"max_length\": 50\n    }\n  },\n  \"transformers_version\": \"4.26.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 50257\n}\n\nloading configuration file /kaggle/working/gpt2_256_full/config.json\nModel config GPT2Config {\n  \"_name_or_path\": \"/kaggle/working/gpt2_256_full\",\n  \"activation_function\": \"gelu_new\",\n  \"architectures\": [\n    \"GPT2LMHeadModel\"\n  ],\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"initializer_range\": 0.02,\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 1024,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 12,\n  \"n_positions\": 1024,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"task_specific_params\": {\n    \"text-generation\": {\n      \"do_sample\": true,\n      \"max_length\": 50\n    }\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.26.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 50257\n}\n\nloading weights file /kaggle/working/gpt2_256_full/pytorch_model.bin\nGenerate config GenerationConfig {\n  \"bos_token_id\": 50256,\n  \"eos_token_id\": 50256,\n  \"transformers_version\": \"4.26.1\"\n}\n\nAll model checkpoint weights were used when initializing GPT2LMHeadModel.\n\nAll the weights of GPT2LMHeadModel were initialized from the model checkpoint at /kaggle/working/gpt2_256_full.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\nloading configuration file /kaggle/working/gpt2_256_full/generation_config.json\nGenerate config GenerationConfig {\n  \"_from_model_config\": true,\n  \"bos_token_id\": 50256,\n  \"eos_token_id\": 50256,\n  \"transformers_version\": \"4.26.1\"\n}\n\nGenerate config GenerationConfig {\n  \"bos_token_id\": 50256,\n  \"do_sample\": true,\n  \"eos_token_id\": 50256,\n  \"max_length\": 50,\n  \"transformers_version\": \"4.26.1\"\n}\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"}]},{"cell_type":"code","source":"# generated_poem","metadata":{"execution":{"iopub.status.busy":"2023-03-15T14:24:33.901658Z","iopub.execute_input":"2023-03-15T14:24:33.902398Z","iopub.status.idle":"2023-03-15T14:24:33.911196Z","shell.execute_reply.started":"2023-03-15T14:24:33.902358Z","shell.execute_reply":"2023-03-15T14:24:33.909941Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"print(generated_poem[0]['generated_text'])\n","metadata":{"execution":{"iopub.status.busy":"2023-03-15T14:24:33.913144Z","iopub.execute_input":"2023-03-15T14:24:33.913555Z","iopub.status.idle":"2023-03-15T14:24:33.923845Z","shell.execute_reply.started":"2023-03-15T14:24:33.913512Z","shell.execute_reply":"2023-03-15T14:24:33.922740Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Criminal, you took a great piece of my life -\nAnd in it you must repay me.\nYou came in a robe of scarlet black,\nAnd at a time of great concern\nYou had my advice, and was ready to answer,\nFor I said no to it;\nIf it be so, then no man will give you any rest,\nNo one will answer me again, if it be so.\nFor you, a fool, who had no sense -\nI cannot tell - but you left behind me a\nSacrament of pain, and a debt unpaid.\nMy Lord, I shall not be sold again -\nNor may my dear Lord receive it from me,\nFor I am the son of a prostitute,\nAnd that he was no longer my Lord was written\nin the book of repentance.\nYour pardon for me must not only be given,\nYour life is mine, as if you loved me -\nYou gave a sacrifice for my sake;\nMy sins were your own.\nOh, it is not so, for he loved and was faithful -\nI, an innocent virgin,\nHad been his bride till you began.\nHe loved me, I was his only love\nFor I love you more than his mother did\nFor he was a faithful wife, and that he loved me\nAnd that he was not deceived by the wine of my wine.\nI did not love, nor did you know that he loved more -\nYou said, \"My poor virgin\" - and I answered:\nAnd this is when I say: - \"I came from the Lord, and that I loved you more\nBecause you did not change me in my sins;\nBecause you are God's son and that you are my heart-mate.\nIf my sins are forgiven, then I will have given you peace,\"\nI gave it like a curse or death.\nOh, when you do my Lord's work in his sight,\nHe will see, and His eyes will see!And when thou in the city of heaven dost live\nThou shalt find a priest and a priestess\nAnd her children in the fields\nAnd she shall bear unto him her mother's child\nThe little one, even the smallest\nAnd she with them the little one\nBut with this shepherdess\nAnd their home shall the world adorn\nAnd all their food, and all their drink.\nAnd that land may well be called the land of gold\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# INFERENCE V2","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import GPT2Tokenizer, AutoModelWithLMHead\n\n# Load the tokenizer and fine-tuned model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = AutoModelWithLMHead.from_pretrained('/kaggle/working/gpt2_256_full')\n\n# Set the device for inference\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n# Define a function for generating poems\ndef generate_poem(prompt, max_length=256, temperature=1.0):\n    # Tokenize the prompt and convert to tensor\n    prompt_tokens = tokenizer.encode(prompt, add_special_tokens=False)\n    prompt_tensor = torch.tensor([prompt_tokens]).to(device)\n\n    # Generate text using the fine-tuned model\n    generated_tokens = model.generate(\n        input_ids=prompt_tensor,\n        max_length=max_length+len(prompt_tokens),\n        temperature=temperature,\n        do_sample=True,\n        num_beams=5,\n        top_k=50,\n        top_p=0.95,\n        repetition_penalty=1.5,\n        no_repeat_ngram_size=2,\n        pad_token_id=tokenizer.eos_token_id\n    )\n\n    # Decode the generated tokens and return the resulting poem\n    poem = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n    return poem\n","metadata":{"execution":{"iopub.status.busy":"2023-03-15T14:17:13.864703Z","iopub.execute_input":"2023-03-15T14:17:13.865202Z","iopub.status.idle":"2023-03-15T14:17:16.384721Z","shell.execute_reply.started":"2023-03-15T14:17:13.865153Z","shell.execute_reply":"2023-03-15T14:17:16.379732Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"loading file vocab.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/vocab.json\nloading file merges.txt from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/merges.txt\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at None\nloading file tokenizer_config.json from cache at None\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\nModel config GPT2Config {\n  \"_name_or_path\": \"gpt2\",\n  \"activation_function\": \"gelu_new\",\n  \"architectures\": [\n    \"GPT2LMHeadModel\"\n  ],\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"initializer_range\": 0.02,\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 1024,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 12,\n  \"n_positions\": 1024,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"task_specific_params\": {\n    \"text-generation\": {\n      \"do_sample\": true,\n      \"max_length\": 50\n    }\n  },\n  \"transformers_version\": \"4.26.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 50257\n}\n\nloading configuration file /kaggle/working/gpt2_256_full/config.json\nModel config GPT2Config {\n  \"_name_or_path\": \"/kaggle/working/gpt2_256_full\",\n  \"activation_function\": \"gelu_new\",\n  \"architectures\": [\n    \"GPT2LMHeadModel\"\n  ],\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"initializer_range\": 0.02,\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 1024,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 12,\n  \"n_positions\": 1024,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"task_specific_params\": {\n    \"text-generation\": {\n      \"do_sample\": true,\n      \"max_length\": 50\n    }\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.26.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 50257\n}\n\nloading weights file /kaggle/working/gpt2_256_full/pytorch_model.bin\nGenerate config GenerationConfig {\n  \"bos_token_id\": 50256,\n  \"eos_token_id\": 50256,\n  \"transformers_version\": \"4.26.1\"\n}\n\nAll model checkpoint weights were used when initializing GPT2LMHeadModel.\n\nAll the weights of GPT2LMHeadModel were initialized from the model checkpoint at /kaggle/working/gpt2_256_full.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\nloading configuration file /kaggle/working/gpt2_256_full/generation_config.json\nGenerate config GenerationConfig {\n  \"_from_model_config\": true,\n  \"bos_token_id\": 50256,\n  \"eos_token_id\": 50256,\n  \"transformers_version\": \"4.26.1\"\n}\n\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = \"There once was a ship that put to sea\"\ngenerated_poem = generate_poem(prompt, max_length=256, temperature=0.7)\nprint(generated_poem)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-15T14:18:29.930226Z","iopub.execute_input":"2023-03-15T14:18:29.930805Z","iopub.status.idle":"2023-03-15T14:18:34.578289Z","shell.execute_reply.started":"2023-03-15T14:18:29.930763Z","shell.execute_reply":"2023-03-15T14:18:34.574940Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"Generate config GenerationConfig {\n  \"bos_token_id\": 50256,\n  \"eos_token_id\": 50256,\n  \"transformers_version\": \"4.26.1\"\n}\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/3295644200.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"There once was a ship that put to sea\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgenerated_poem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_poem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_poem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/3751585683.py\u001b[0m in \u001b[0;36mgenerate_poem\u001b[0;34m(prompt, max_length, temperature)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mno_repeat_ngram_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mpad_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     )\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1520\u001b[0m                 \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m                 \u001b[0msynced_gpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1523\u001b[0m             )\n\u001b[1;32m   1524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mbeam_sample\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3094\u001b[0m             \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3096\u001b[0;31m             \u001b[0mnext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3097\u001b[0m             \u001b[0mnext_token_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"],"ename":"RuntimeError","evalue":"probability tensor contains either `inf`, `nan` or element < 0","output_type":"error"}]},{"cell_type":"code","source":"!zip -r gpt2_256 '/kaggle/working/gpt2_256_full'","metadata":{"execution":{"iopub.status.busy":"2023-03-15T14:17:28.605790Z","iopub.execute_input":"2023-03-15T14:17:28.606837Z","iopub.status.idle":"2023-03-15T14:17:56.281680Z","shell.execute_reply.started":"2023-03-15T14:17:28.606798Z","shell.execute_reply":"2023-03-15T14:17:56.280390Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"  adding: kaggle/working/gpt2_256_full/ (stored 0%)\n  adding: kaggle/working/gpt2_256_full/generation_config.json (deflated 24%)\n  adding: kaggle/working/gpt2_256_full/pytorch_model.bin (deflated 9%)\n  adding: kaggle/working/gpt2_256_full/config.json (deflated 51%)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}